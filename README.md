# Data Engineer Intern â€“ Website Data Pipeline

## ğŸ“Œ Overview

This project implements a **production-style end-to-end data engineering pipeline** to crawl websites, extract structured content, standardize it into a clean data model, and generate analytical insights.

The pipeline follows a **real-world data lake architecture** and is orchestrated using **Apache Airflow** for scheduling and reliability.

### Key Objectives

* Demonstrate data pipeline design
* Show modular and clean code structure
* Implement reliable orchestration
* Build scalable architecture

---

## ğŸ— Project Architecture

```
Growthpal-Pipeline/
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ website_pipeline_dag.py     # Airflow DAG (orchestration layer)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ crawler.py                 # Website crawling logic
â”‚   â”œâ”€â”€ extractor.py               # HTML parsing & tagging
â”‚   â”œâ”€â”€ transformer.py             # Standard data model creation
â”‚   â””â”€â”€ aggregator.py              # Analytics & metrics
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # Raw HTML (S3 simulation)
â”‚   â”œâ”€â”€ processed/                # Clean structured data
â”‚   â””â”€â”€ analytics/                # Aggregated metrics
â”‚
â”œâ”€â”€ run_pipeline.py                # Local pipeline runner (without Airflow)
â”œâ”€â”€ venv/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”„ Pipeline Flow

### 1ï¸âƒ£ Website Crawling

* Fetch raw HTML using `requests`
* Capture metadata (URL, status code, crawl timestamp)
* Store raw files in `data/raw/`
  *(Simulates S3 raw storage layer)*

### 2ï¸âƒ£ Content Extraction

Using **BeautifulSoup**, extract:

* Navbar content
* Homepage content
* Footer content
* Case study links (heuristic based)

### 3ï¸âƒ£ Data Transformation

Convert extracted content into a **standard JSON format**:

```json
{
  "website": "https://example.com",
  "section": "homepage",
  "content": "Extracted text...",
  "crawl_timestamp": "2026-01-10T10:30:00Z",
  "isActive": true
}
```

Each website generates multiple records (one per section).

### 4ï¸âƒ£ Aggregation & Metrics

Compute:

* Number of websites with case studies
* Content length statistics per section

### 5ï¸âƒ£ Orchestration (Apache Airflow)

* Modular task design
* Retry enabled for failures
* Idempotent execution
* Easily extendable to new websites

---

## âš™ Installation & Setup

### 1ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Run Pipeline Locally (Without Airflow)

```bash
python run_pipeline.py
```

This generates:

* Raw HTML â†’ `data/raw/`
* Structured data â†’ `data/processed/structured.json`
* Metrics â†’ `data/analytics/metrics.json`

### 4ï¸âƒ£ Run with Airflow (Optional)

```bash
airflow db init

airflow users create \
  --username admin \
  --password admin \
  --firstname admin \
  --lastname admin \
  --role Admin \
  --email admin@test.com

airflow webserver -p 8080
airflow scheduler
```

Open:

```
http://localhost:8080
```

Trigger DAG:
**growthpal_pipeline**

---

## ğŸ§  Design Decisions

| Area                   | Reason                     |
| ---------------------- | -------------------------- |
| Modular code           | Easy maintenance & testing |
| Raw â†’ Processed layers | Follows data lake pattern  |
| Heuristic scraping     | Focus on pipeline design   |
| JSON output            | API & analytics ready      |
| Airflow orchestration  | Production scheduling      |

---

## ğŸ›¡ Failure Handling

* Network timeout handling
* Retry enabled in Airflow
* Failed websites skipped safely
* Logs available for debugging

---

## ğŸš€ Scalability & Future Enhancements

* Parallel crawling using async processing
* S3 integration for storage
* Spark for big data processing
* API-based ingestion
* Dynamic Airflow task generation

---

## ğŸ“Š Sample Outputs

* `data/raw/` â†’ raw HTML files
* `data/processed/structured.json` â†’ clean structured data
* `data/analytics/metrics.json` â†’ analytical metrics

---

## ğŸ‘¨â€ğŸ’» Author

**Damodar Sadavarte**
Software Engineer | Data Analytics | AI & ML Engineer

ğŸ“§ Email: [damodarsadavarte2000@gmail.com](mailto:damodarsadavarte2000@gmail.com)
ğŸ”— GitHub: [https://github.com/ThunderDamo1606](https://github.com/ThunderDamo1606)
ğŸ”— LinkedIn: [https://linkedin.com/in/damodar-sadavarte](https://linkedin.com/in/damodar-sadavarte)

---

## ğŸ Conclusion

This project demonstrates:

* Real-world data engineering workflow
* Clean & scalable architecture
* Production-ready design
* Strong understanding of ETL pipelines

---

â­ Thank you for reviewing!
