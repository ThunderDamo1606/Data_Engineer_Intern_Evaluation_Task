# Data_Engineer_Intern_Evaluation_Task
## Website Data Pipeline

## ğŸ“Œ Overview

This project implements an end-to-end data engineering pipeline to crawl websites, extract structured content, standardize the data, and compute basic analytics.
The pipeline is orchestrated using **Apache Airflow** and follows real-world data lake design patterns.

The main goal is to demonstrate:

* Data pipeline design
* Clean code structure
* Reliable orchestration
* Scalable architecture

---

## ğŸ— Project Architecture

```
Growthpal-Pipeline/
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ website_pipeline_dag.py
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ crawler.py
â”‚   â”œâ”€â”€ extractor.py
â”‚   â”œâ”€â”€ transformer.py
â”‚   â””â”€â”€ aggregator.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ analytics/
â”‚
â”œâ”€â”€ venv/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”„ Pipeline Flow

### 1ï¸âƒ£ Crawl Websites

* Fetch raw HTML using `requests`
* Capture metadata like URL and crawl timestamp
* Store raw files in `data/raw/` (S3 simulation)

### 2ï¸âƒ£ Extract Content

Using BeautifulSoup:

* Navbar content
* Homepage content
* Footer content
* Case study links (heuristic based)

### 3ï¸âƒ£ Transform

Convert extracted content into standard JSON structure:

```json
{
  "website": "https://example.com",
  "section": "homepage",
  "content": "Extracted text...",
  "crawl_timestamp": "2026-01-10T10:30:00Z",
  "isActive": true
}
```

### 4ï¸âƒ£ Aggregate

Compute metrics:

* Number of websites with case studies
* Content length statistics per section

### 5ï¸âƒ£ Orchestration (Airflow)

* Modular tasks
* Retry enabled
* Idempotent execution
* Easy to extend for new websites

---

## âš™ Installation & Setup

### 1. Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Start Airflow (Local)

```bash
airflow db init
airflow users create \
  --username admin \
  --password admin \
  --firstname admin \
  --lastname admin \
  --role Admin \
  --email admin@test.com

airflow webserver -p 8080
airflow scheduler
```

Open:

```
http://localhost:8080
```

Trigger DAG:
**growthpal_pipeline**

---

## ğŸ§  Design Decisions

| Area                  | Reason              |
| --------------------- | ------------------- |
| Modular code          | Easy maintenance    |
| Raw/Processed layers  | Data lake pattern   |
| Heuristic scraping    | Focus on pipeline   |
| JSON output           | API ready           |
| Airflow orchestration | Production workflow |

---

## ğŸ›¡ Failure Handling

* Network timeout handling
* Airflow retries enabled
* Skip failed websites
* Logs for debugging

---

## ğŸš€ Scalability

Future improvements:

* Parallel crawling
* S3 storage
* Spark processing
* API ingestion
* Dynamic Airflow tasks

---

## ğŸ“Š Sample Outputs

* `data/raw/` â†’ raw HTML files
* `data/processed/structured.json` â†’ clean data
* `data/analytics/metrics.json` â†’ analytics

---

## ğŸ‘¨â€ğŸ’» Author

**Damodar Sadavarte**
Software Engineer | Data Analytics | AI & ML Engineer

ğŸ“§ Email: [damodarsadavarte2000@gmail.com](mailto:damodarsadavarte2000@gmail.com)
ğŸ”— GitHub: [https://github.com/ThunderDamo1606](https://github.com/ThunderDamo1606)
ğŸ”— LinkedIn: [https://linkedin.com/in/damodar-sadavarte](https://linkedin.com/in/damodar-sadavarte)

---

## ğŸ Conclusion

This project demonstrates:

* Real data engineering practices
* Clean architecture
* Scalable design
* Production mindset

---

â­ Thank you for reviewing!
