# Website Data Pipeline â€“ Data Engineer Intern Project Task

## Overview

This project demonstrates an **end-to-end data engineering pipeline** designed to crawl multiple websites, extract meaningful content, transform it into a structured format, and generate analytical insights.

The solution reflects **real-world data engineering practices**, including modular code design, data layering, logging, and workflow orchestration.
The pipeline supports both **local execution** and **production-style orchestration using Apache Airflow**.

---

## Project Architecture

```
Growthpal-Pipeline/
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ website_pipeline_dag.py      # Airflow DAG definition
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ crawler.py                   # Website crawling logic
â”‚   â”œâ”€â”€ extractor.py                 # HTML parsing & content extraction
â”‚   â”œâ”€â”€ transformer.py               # Standardized data transformation
â”‚   â”œâ”€â”€ aggregator.py                # Metrics & analytics computation
â”‚   â””â”€â”€ logger.py                    # Centralized logging configuration
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                         # Raw HTML (data lake â€“ raw layer)
â”‚   â”œâ”€â”€ processed/                  # Cleaned & structured data
â”‚   â””â”€â”€ analytics/                  # Aggregated metrics & insights
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_extractor.py            # Basic unit test
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ pipeline.log                 # Pipeline execution logs
â”‚
â”œâ”€â”€ run_pipeline.py                  # Local pipeline runner
â”œâ”€â”€ websites.txt                    # Input websites list
â”œâ”€â”€ requirements.txt                # Project dependencies
â””â”€â”€ README.md
```

---

## Pipeline Workflow

### 1. Website Crawling

* Fetches website HTML using `requests`
* Captures metadata such as URL, status code, and crawl timestamp
* Persists raw HTML to simulate a **raw data layer**

```
data/raw/<website_name>/homepage.html
```

---

### 2. Content Extraction

HTML content is parsed using **BeautifulSoup**, and the following sections are extracted:

* Navigation bar
* Main homepage content
* Footer
* Case study links (heuristic based)

---

### 3. Data Transformation

Extracted content is normalized into a **standard JSON schema**:

```json
{
  "website": "https://example.com",
  "section": "homepage",
  "content": "Extracted text...",
  "crawl_timestamp": "2026-01-10T10:30:00Z",
  "isActive": true
}
```

Each website generates multiple records, one per extracted section.

---

### 4. Aggregation & Analytics

The pipeline computes high-level metrics, including:

* Number of websites containing case studies
* Content length statistics (minimum, maximum, average) by section

Metrics output location:

```
data/analytics/metrics.json
```

---

## Installation & Setup

### Create Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate
```

### Install Dependencies

```bash
pip install -r requirements.txt
```

---

## Running the Pipeline

### Local Execution

Run the complete pipeline without orchestration:

```bash
python run_pipeline.py
```

Generated outputs:

* Raw HTML â†’ `data/raw/`
* Structured data â†’ `data/processed/structured.json`
* Metrics â†’ `data/analytics/metrics.json`
* Logs â†’ `logs/pipeline.log`

---

### Apache Airflow Orchestration

The project also includes an Airflow DAG for scheduled execution and monitoring.

**Steps:**

1. Initialize Airflow database
2. Create an admin user
3. Start Airflow services
4. Trigger DAG: `growthpal_pipeline`

---

## Configuration

### websites.txt

Defines the list of target websites:

```
https://openai.com
https://shopify.com
https://stripe.com
```

The pipeline dynamically processes all websites listed here.

---

## Key Design Decisions

| Component            | Rationale                            |
| -------------------- | ------------------------------------ |
| Modular architecture | Easier testing & maintainability     |
| Raw â†’ processed flow | Industry-standard data lake pattern  |
| JSON format          | Analytics-ready & API-friendly       |
| Central logging      | Simplified debugging & observability |
| Airflow DAG          | Production-grade orchestration       |

---

## Error Handling & Reliability

* Network timeout handling during crawling
* Graceful skipping of failed websites
* Centralized logging for traceability
* Retry logic enabled at orchestration level

---

## Scalability & Future Enhancements

* Asynchronous crawling
* Cloud storage (S3 / GCS) integration
* Distributed processing (Spark)
* Dynamic Airflow DAG generation
* Monitoring & alerting

---

## Author

**Damodar Sadavarte**

- Software Engineer  
- Data Analyst  
- AI & ML Engineer  

ğŸ“§ **Email:** damodarsadavarte2000@gmail.com  
ğŸ”— **GitHub:** https://github.com/ThunderDamo1606  
ğŸ”— **LinkedIn:** https://linkedin.com/in/damodar-sadavarte

---

## Summary

This project highlights:

* Practical data engineering skills
* Clean, production-oriented architecture
* End-to-end ETL pipeline implementation
* Workflow orchestration using Airflow

â­ Thank you for reviewing!
